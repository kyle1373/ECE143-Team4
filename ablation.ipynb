{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\24543\\anaconda3\\envs\\env1\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:975: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Accuracy: 0.9635443037974684\n",
      "Gradient Boosting Accuracy: 0.8126582278481013\n",
      "\n",
      "Random Forest Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.94      0.93       497\n",
      "           1       0.94      0.92      0.93       498\n",
      "           2       1.00      1.00      1.00       483\n",
      "           3       1.00      1.00      1.00       497\n",
      "\n",
      "    accuracy                           0.96      1975\n",
      "   macro avg       0.96      0.96      0.96      1975\n",
      "weighted avg       0.96      0.96      0.96      1975\n",
      "\n",
      "\n",
      "Gradient Boosting Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.63      0.67       497\n",
      "           1       0.69      0.71      0.70       498\n",
      "           2       0.83      0.92      0.87       483\n",
      "           3       1.00      1.00      1.00       497\n",
      "\n",
      "    accuracy                           0.81      1975\n",
      "   macro avg       0.81      0.81      0.81      1975\n",
      "weighted avg       0.81      0.81      0.81      1975\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.utils import resample\n",
    "\n",
    "data = pd.read_csv('numeric_data.csv')\n",
    "\n",
    "# Define categorical columns (all columns except the target column)\n",
    "target_column = 'How_is_curret_days_parenting'  # Replace with your target column name\n",
    "categorical_columns = [col for col in data.columns if col != target_column]\n",
    "\n",
    "# One-hot encoding for categorical columns\n",
    "encoder = OneHotEncoder(sparse=False, handle_unknown='ignore')\n",
    "encoded_data = encoder.fit_transform(data[categorical_columns])  # Specify your categorical columns\n",
    "encoded_columns = encoder.get_feature_names_out(categorical_columns)\n",
    "encoded_df = pd.DataFrame(encoded_data, columns=encoded_columns)\n",
    "\n",
    "data_encoded = pd.concat([data.drop(categorical_columns + [target_column], axis=1), encoded_df], axis=1)\n",
    "\n",
    "# Label encoding for the target variable\n",
    "label_encoder = LabelEncoder()\n",
    "data_encoded['Target_Encoded'] = label_encoder.fit_transform(data[target_column])\n",
    "\n",
    "# Balancing the dataset\n",
    "class_distribution = data_encoded['Target_Encoded'].value_counts()\n",
    "max_size = class_distribution.max()\n",
    "\n",
    "balanced_data = pd.DataFrame()\n",
    "for cls in data_encoded['Target_Encoded'].unique():\n",
    "    class_subset = resample(data_encoded[data_encoded['Target_Encoded'] == cls],\n",
    "                            replace=True, \n",
    "                            n_samples=max_size, \n",
    "                            random_state=42)\n",
    "    balanced_data = pd.concat([balanced_data, class_subset], axis=0)\n",
    "\n",
    "# Splitting the dataset\n",
    "X = balanced_data.drop('Target_Encoded', axis=1)\n",
    "y = balanced_data['Target_Encoded']\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Model training and evaluation\n",
    "rf_classifier = RandomForestClassifier(random_state=42)\n",
    "gb_classifier = GradientBoostingClassifier(random_state=42)\n",
    "\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "gb_classifier.fit(X_train, y_train)\n",
    "\n",
    "# validation\n",
    "y_pred_rf = rf_classifier.predict(X_val)\n",
    "y_pred_gb = gb_classifier.predict(X_val)\n",
    "\n",
    "accuracy_rf = accuracy_score(y_val, y_pred_rf)\n",
    "accuracy_gb = accuracy_score(y_val, y_pred_gb)\n",
    "\n",
    "# performance\n",
    "print(\"Random Forest Accuracy:\", accuracy_rf)\n",
    "print(\"Gradient Boosting Accuracy:\", accuracy_gb)\n",
    "print(\"\\nRandom Forest Classification Report:\\n\", classification_report(y_val, y_pred_rf))\n",
    "print(\"\\nGradient Boosting Classification Report:\\n\", classification_report(y_val, y_pred_gb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary to store accuracy without each feature\n",
    "accuracy_without_feature_gb = {}\n",
    "accuracy_without_feature_rf = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iterate 0 times.\n"
     ]
    }
   ],
   "source": [
    "# Iterate over each feature\n",
    "it_numbers = 0\n",
    "for feature in X_train.columns:\n",
    "    X_train_dropped = X_train.drop(feature, axis=1)\n",
    "    X_test_dropped = X_val.drop(feature, axis=1)\n",
    "\n",
    "    # Retrain the model\n",
    "    rf_classifier = RandomForestClassifier(random_state=42)\n",
    "    gb_classifier = GradientBoostingClassifier(random_state=42)\n",
    "    rf_classifier.fit(X_train_dropped, y_train)\n",
    "    gb_classifier.fit(X_train_dropped, y_train)\n",
    "    y_pred_dropped_gb = gb_classifier.predict(X_test_dropped)\n",
    "    y_pred_dropped_rf = rf_classifier.predict(X_test_dropped)\n",
    "\n",
    "    # Store the accuracy\n",
    "    accuracy_without_feature_gb[feature] = accuracy_score(y_val, y_pred_dropped_gb)\n",
    "    accuracy_without_feature_gb[feature] = accuracy_score(y_val, y_pred_dropped_rf)\n",
    "    \n",
    "    if it_numbers % 20 == 0:\n",
    "        print(\"iterate {} times.\".format(it_numbers))\n",
    "    \n",
    "    it_numbers += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nAccuracy without each feature:\")\n",
    "for feature, accuracy in accuracy_without_feature_gb.items():\n",
    "    print(f\"{feature}: {accuracy}\")\n",
    "for feature, accuracy in accuracy_without_feature_rf.items():\n",
    "    print(f\"{feature}: {accuracy}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
